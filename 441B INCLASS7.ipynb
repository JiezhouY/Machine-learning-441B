{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKSTLF2BX6jH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N11Ee3GJmywu",
        "outputId": "79137ea6-daa5-424f-f209-02d8cab5cb43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=23821e3a6b64dd86529b7c69593a52b116362a97367ac4b2d18c798164830633\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: h11, wikipedia, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0 wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import wikipedia"
      ],
      "metadata": {
        "id": "Q2A8TGhKm3i5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.) Set up OpenAI and the enviornment\n"
      ],
      "metadata": {
        "id": "7E9HEMJSX-3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apikey = \"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\""
      ],
      "metadata": {
        "id": "4zwwdkZDYDZN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client= openai.OpenAI(\n",
        "    api_key=apikey\n",
        ")"
      ],
      "metadata": {
        "id": "8IiKS0snlpYP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSL--dhXMvnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
      ],
      "metadata": {
        "id": "tOXc5_BTm9HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_titles= [\"Artificial\", \"Intelligent\", \"UCLA\"]"
      ],
      "metadata": {
        "id": "jFEF-h6tNJgh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_title =page_titles[0]"
      ],
      "metadata": {
        "id": "ZEBlMc-VNZmk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_result =wikipedia.search(page_title)"
      ],
      "metadata": {
        "id": "upFPbkr3Nd7D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page = wikipedia.search(search_result[0])"
      ],
      "metadata": {
        "id": "nz0bYaIsNuqP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikipedia_content(page_title):\n",
        "  search_result =wikipedia.search(page_title)\n",
        "  page = wikipedia.page(search_result[0])\n",
        "  return(page.content)"
      ],
      "metadata": {
        "id": "Bde_I0LWN04d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = get_wikipedia_content(page_title)"
      ],
      "metadata": {
        "id": "yx2AVkvLOZAB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  dir(wikipedia)\n"
      ],
      "metadata": {
        "id": "-v7OYamHlrEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd73feb-ab17-42b2-e3cc-314e3e99e70d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['API_URL',\n",
              " 'BeautifulSoup',\n",
              " 'Decimal',\n",
              " 'DisambiguationError',\n",
              " 'HTTPTimeoutError',\n",
              " 'ODD_ERROR_MESSAGE',\n",
              " 'PageError',\n",
              " 'RATE_LIMIT',\n",
              " 'RATE_LIMIT_LAST_CALL',\n",
              " 'RATE_LIMIT_MIN_WAIT',\n",
              " 'RedirectError',\n",
              " 'USER_AGENT',\n",
              " 'WikipediaException',\n",
              " 'WikipediaPage',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " 'cache',\n",
              " 'datetime',\n",
              " 'debug',\n",
              " 'donate',\n",
              " 'exceptions',\n",
              " 'geosearch',\n",
              " 'languages',\n",
              " 'page',\n",
              " 'random',\n",
              " 're',\n",
              " 'requests',\n",
              " 'search',\n",
              " 'set_lang',\n",
              " 'set_rate_limiting',\n",
              " 'set_user_agent',\n",
              " 'stdout_encode',\n",
              " 'suggest',\n",
              " 'summary',\n",
              " 'sys',\n",
              " 'time',\n",
              " 'timedelta',\n",
              " 'unicode_literals',\n",
              " 'util',\n",
              " 'wikipedia']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgY2FkTdmhTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kw5H5jMlmmS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZF3BiZyXltYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ef7yfa2jl0iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
      ],
      "metadata": {
        "id": "_9aruncMmubX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completions = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a summary assistant at Wikipedia, I will pass you an article and please tell me if any of the information is false\"},\n",
        "        {\"role\": \"user\", \"content\": content[:8180]}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "1tWvl0UnTxfE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TMKFGN4nDJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_error_correction(test):\n",
        "    chat_completions = client.chat.completions.create(\n",
        "    model = 'gpt-4',\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'I will be giving you an article. I am looking for false information. I want to capture all potentially false info, if there is even small potential for it to be wrong, please return it. PleaseIf there is no false information only return Done!'},\n",
        "        {'role': 'user', 'content': content[:8180]}]\n",
        "    )\n",
        "    print(chat_completions.choices[0].message.content)"
      ],
      "metadata": {
        "id": "6FKAJVXSoayA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
      ],
      "metadata": {
        "id": "zPw5LyPEobmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_titles= [\"Artificial Intelligent\", \"UCLA\",\"Rain\"]"
      ],
      "metadata": {
        "id": "V7cuhML2ocGn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for page_title in page_titles:\n",
        "  try:\n",
        "    print(\"________________\"+page_title)\n",
        "    content = get_wikipedia_content(page_title)\n",
        "    chatgpt_error_correction(content)\n",
        "  except:\n",
        "    print(\"ERROR\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrwlIoIjRuVo",
        "outputId": "51bfd192-a62e-4456-9393-f61c1ead5557"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________________Artificial Intelligent\n",
            "\"Alan Turing was the first person to conduct substantial research in the field that he called machine intelligence.\" - This is misleading. While Alan Turing contributed significantly to theoretical concepts that laid the foundation for AI, he was not the first person to conduct substantial research in the field. There were several others who studied and contributed to it around the same time or even before Turing.\n",
            "\"The field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter.\" - This term, AI winter, needs clarification. While it's true that there were periods of reduced interest and funding in AI research, the term \"AI winter\" is often used with the implication of a total cessation or deep freeze in AI research, which is not accurate.\n",
            "\"Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture.\" - This statement oversimplifies the advancements in AI and implies that deep learning and transformer architecture superseded all previous techniques. Multiple AI techniques have advanced simultaneously, and older techniques can be and are still used effectively. \n",
            "\"This led to the AI spring of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.\" - This may be overstating the United States' influence in the current AI research landscape. Significant advances in AI come from global sources.\n",
            "\"Accurate and efficient reasoning is an unsolved problem.\" - This statement suggests that no progress has been made in developing accurate and efficient reasoning in AI, which is not accurate. While it is true that creating AI with human-level reasoning ability remains a challenge, substantial advancements have occurred.\n",
            "\"Knowledge bases need to represent things such as: objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\" - While these are important aspects of a knowledge base, it's inaccurate to imply that they must all be represented in every knowledge base. The required representation depends on the specific use case of the AI system.\n",
            "\"A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way, and a reward function that supplies the utility of each state and the cost of each action.\" - Not all Markov decision processes include a reward function that measures the utility of both the state and the action. The reward function typically only supplies the utility of each state.\n",
            "________________UCLA\n",
            "1. Its academic roots were established in 1881 as a normal school then known as the southern branch of the California State Normal School which later evolved into San José State University.\n",
            "2. The branch was transferred to the University of California, becoming the Southern Branch of UC in 1919, making it the second-oldest of the ten-campus University of California system after the University of California, Berkeley.\n",
            "3. UCLA offers 337 undergraduate and graduate degree programs in a range of disciplines, enrolling about 31,600 undergraduate and 14,300 graduate and professional students annually.\n",
            "4. It received 174,914 undergraduate applications for Fall 2022, including transfers, making it the most applied-to university in the United States.\n",
            "5. They have won 121 NCAA team championships, second only to Stanford University's 128 team titles.\n",
            "6. 410 Bruins have made Olympic teams, winning 270 Olympic medals: 136 gold, 71 silver and 63 bronze.\n",
            "7. UCLA has been represented in every Olympics since the university's founding (except in 1924) and has had a gold medalist in every Olympics in which the U.S. has participated since 1932.\n",
            "8. As of October 2021, 27 Nobel laureates, five Turing Award winners, two Chief Scientists of the U.S. Air Force and one Fields Medalist have been affiliated with it as faculty, researchers and alumni.\n",
            "9. As of August 2021, 55 associated faculty members have been elected to the National Academy of Sciences, 29 to the National Academy of Engineering, 41 to the National Academy of Medicine and 156 to the American Academy of Arts and Sciences.\n",
            "10. The university was elected to the Association of American Universities in 1974.\n",
            "11. The Regents announced the new \"Beverly Site\" — just west of Beverly Hills — in 1925.\n",
            "12. After the athletic teams entered the Pacific Coast conference in 1926, the Southern Branch student council adopted the nickname \"Bruins\", a name offered by the student council at UC Berkeley.\n",
            "13. On February 1, 1927, the Regents renamed the Southern Branch the University of California at Los Angeles.\n",
            "14. The campus in Westwood opened to students in 1929.\n",
            "15. The first undergraduate classes on the new campus were held in 1929 with 5,500 students.\n",
            "16. UCLA was permitted to award the master's degree in 1933, and the doctorate in 1936, against continued resistance from UC Berkeley.\n",
            "17. In 1951, UCLA was formally elevated to co-equal status with UC Berkeley, and its presiding officer Raymond B. Allen was the first chief executive to be granted the title of chancellor.\n",
            "18. On June 1, 2016, two men were killed in a murder-suicide at an engineering building in the university.\n",
            "19. In February 2022, Matthew Harris, a former lecturer and postdoctoral fellow at UCLA, was arrested after allegedly making numerous threats of violence against students and faculty members of UCLA's Philosophy Department.\n",
            "20. In 2018, a student-led community coalition known as \"Westwood Forward\" successfully led an effort to break UCLA and Westwood Village away from the existing Westwood Neighborhood Council and form a new North Westwood Neighborhood Council, with over 2,000 out of 3,521 stakeholders voting in favor of the split.\n",
            "21. In 2022, UCLA signed an agreement to partner with the Tongva for the caretaking and landscaping of various areas of the campus.\n",
            "22. In 2014, a graduate student adviser and professor in the history department, Gabriel Piterberg, was accused of sexually assaulting two students.\n",
            "________________Rain\n",
            "1. The steam locomotive was not invented in the United Kingdom in 1802. George Stephenson is credited with developing the first successful steam locomotive in 1814. \n",
            "2. Trains did not quickly spread around the world after the invention of the steam locomotive in 1802 as suggested, there was a gradual evolution and adaptation of railway systems.\n",
            "3. The steam locomotive by Richard Trevithick did not power the first ever steam train in 1804. His invention only demonstrated the possibility of using steam power for a railway.\n",
            "4. The first steam railroad in the United States did not open in 1829. The first full-scale working railway steam locomotive was built in the United Kingdom in 1804 by Richard Trevithick.\n",
            "5. The first steam train did not run in France in late 1829. The first steam-powered railway journey took place in The United Kingdom in the early 19th century.\n",
            "6. The claim that the first train powered by electricity was built by Werner von Siemens in 1879 is incorrect. Thomas Davenport, an American inventor, built a small battery-powered electric locomotive in 1835.\n",
            "7. Diesel engines were not recognized as a potential power source for trains decades after their invention in the 1890s. The first diesel-powered locomotive was built in 1912.\n",
            "8. The claim might not be accurate that Italy was driven to develop an extensive network of electric trains due to lack of significant coal reserves in the early 20th century.\n",
            "9. The statement saying steam locomotives were used to haul mainline trains as late as 2005 in Inner Mongolia, China may be incorrect. Major railways had ceased to use steam locomotives by the early 2000s.\n",
            "10. The claim that diesel and electric locomotives replaced steam as the means of motive power beginning in the 1920s could be incorrect. The Diesel revolution in railroads did not take place until after WWII, in the mid-20th century. Most steam locomotives were not replaced until the 1950s and 1960s.\n",
            "11. The statement that trams were first built in the late 1800s to transport large numbers of people in and around cities might be inaccurate. The first tram was built in 1807, in the early 19th century, in New York. \n",
            "12. The part that refers to the Marshal Plan after WWII might be incorrect. The Marshall Plan was specific to Western Europe, and did not fund countries behind the Iron Curtain.\n",
            "13. The assertion that the majority of the world's steam locomotives had been retired by 1980 may need verification. In some parts of the world, steam locomotives continued to be used into the 21st Century. \n",
            "14. The sentence stating railroads were operating on every continent besides uninhabited Antarctica by 1900 might be incorrect. Antarctica does have the Grand Duchy of Westarctica's Polar Express, a light rail line used to transport supplies and personnel among various stations, though it was not operational by 1900. \n",
            "15. The claim about high-speed rail traveling at speeds of 240 kilometers per hour (150 mph) or greater starting from the 1960s could be misleading. The Shinkansen, the first high-speed rail, had a maximum speed of 210 km/h (130 mph) when it was introduced in 1964. The speed of 240 km/h was not achieved until later years.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kza1P4D4RuXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}